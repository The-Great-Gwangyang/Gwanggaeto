### 3장 하둡 분산 파일시스템
+ 분산 파일 시스템 : 네트워크로 연결된 여러 머신의 스토리지를 관리하는 파일 시스템
   <br>→ 일반적인 디스크 파일시스템보다 복잡
+ HDFS(Hadoop Distributed File System) : 하둡의 대표적인 파일 시스템  

#### 3.1 HDFS 설계
+ HDFS의 특징
  - <B style="color:skyblue">매우 큰 파일</B> : 수백 메가바이트, 기가바이트 또는 테라바이트 크기의 파일을 의미, 최근에는 페타바이트 크기의 데이터를 저장하는 하둡클러스터도 존재
  - <B style="color:skyblue">스트리밍 방식의 데이터 접근</B> : HDFS는 가장 효율적인 데이터 처리 패턴은 한 번 쓰고 여러번 읽는 것이라는 아이디어에서 출발했고, 분석이 전부는 아니지만 첫번째 레코드를 읽는데 걸리는 시간보다 전체 데이터셋을 모두 읽을때 걸리는 시간이 더 중요
  - <B style="color:skyblue">범용 하드웨어</B> : 하둡은 고가의 신뢰도 높은 하드웨어만을 고집하지 않고, 범용 하드웨어로 구성된 대형 클러스터에서 장애가 발생해도 사용자가 그 사실을 모르게 작업을 수행하도록 설계됨
+ HDFS가 맞지 않는 응용분야
  - <B style="color:skyblue">빠른 데이터 응답 시간</B> : HDFS는 높은 데이터 처리량을 제공하기 위해 최적화되있고 이를 위해 빠른 응답시간을 포기
  - <B style="color:skyblue">수많은 작은 파일</B> : 네임노드는 파일시스템의 메타데이터를 메모리에서 관리하여 저장가능한 파일 수는 네임노드의 메모리 용량에 좌우되어 수십억 개의 파일은 하드웨어의 용량을 넘어서게 됨
  - <B style="color:skyblue">다중 라이터와 파일의 임의 수정</B> : HDFS는 단일 라이터로 파일을 쓰고, 한번 쓰고 끝나거나 파일의 끝에 덧붙이는 것은 가능하지만 파일에서 임의 위치에 있는 내용을 수정하는 것은 허용하지 않으며 다중 라이터도 지원하지 않음(하둡 3.0부터는 다중 라이터 지원)

#### 3.2 HDFS 개념
1. 블록
   + 블록 크기 : 한 번에 읽고 쓸 수 있는 데이터의 최대량
     - 파일시스템의 블록 크기는 보통 수 kb 고, 디스크 블록의 크기는 기본적으로 512 byte
     - HDFS 블록 크기는 기본적으로 128 MB로 굉장히 큰 단위
   + HDFS 파일은 특정 블록 크기의 청크로 쪼개지고 각 청크는 독립적으로 저장됨
   + 단일 디스크를 위한 파일 시스템은 디스크 블록 크기보다 작은 데이터라도 한 블록 전체를 점유하지만, HDFS는 모두 점유하지 않음
   + HDFS 블록이 큰 이유 : 탐색 비용을 최소화하기 위해서
     - 블록이 매우 크면 블록의 시작점을 탐색하는데 걸리는 시간을 줄이고 데이터를 전송하는데 더 많은 시간 할애 가능
   + 분산 파일 시스템에 블록 추상화의 개념 도입시 얻게되는 이득
        >   1. 파일 하나의 크기가 단일 디스크의 용량보다 커짐
        >       - 한 파일을 구성하는 여러 블록이 동일 디스크에만 저장될 필요가 없음
        >   2. 스토리지의 서브시스템을 단순하게 만들 수 있음 
        >       - 블록은 고정크기고 저장에 필요한 디스크 용량만 계산하면 됨
        >       - 블록은 단지 저장된 데이터의 청크일뿐이고 메타데이터는 별도 시스템으로 분리 가능
   + 블록은 내고장성(fault tolerance)과 가용성(availability)을 제공하는데 필요한 복제(replication) 구현시 적합
      - 블록의 손상과 디스크 및 머신의 장애에 대처하기 위해 각 블록은 물리적으로 분리된 다수의 머신(보통 3개)에 복제됨
      - 블록이나 머신이 손상되면 다른 복사본을 살아있는 머신에 복제하여 복제 계수를 정상수준으로 돌아오게 함
2. 네임노드와 데이터노드
   + HDFS는 마스터인 하나의 네임노드(namenode)와 워커인 여러 데이터노드(datanode)로 구성
   + 네임노드 : 파일시스템의 네임스페이스 관리
      - 파일시스템 트리와 그 트리에 포함된 모든 파일과 디렉터리에 대한 메타데이터 유지
      - 파일에 속한 모든 블록이 어느 데이터 노드에 있는지 파악
      - but, 블록의 위치 정보는 시스템이 시작할때 모든 데이터노드로부터 받아서 재구성하기 때문에 디스크에 영속적으로 저장되진 않음
   + HDFS 클라이언트는 네임노드와 데이터노드 사이에서 통신하고 파일시스템에 접근
   + 데이터노드 : 파일시스템의 실질적인 일꾼
      - 클라이언트나 네임노드의 요청이 있을때 블록을 저장하고 탐색하며, 저장된 블록 목록을 주기적으로 네임노드에 보고
   +  네임노드가 없으면 파일시스템은 동작하지 않고 네임노드를 실행하는 머신이 손상되면 파일시스템의 어떤 파일도 못찾음
     <br>→ 데이터노드에 블록이 저장되있지만 이런 블록 정보를 이용해 파일을 재구성할순 없음
     <br>→ 네임노드의 장애복구 기능은 필수적이며, 하둡은 이를 위해 두가지 매커니즘 제공
      >  1. 파일시스템의 메타데이터를 지속적인 상태로 보존하기 위해 파일로 백업
      >  2. 보조 네임노드 운영
      >     - 보조 네임노드의 역할
      >         *  에디트 로그가 너무 커지지 않도록 주기적으로 네임스페이스 이미지를 에디트 로그와 병합하여 새로운 네임스페이스를 만드는것
      >            <br>→ 병합작업을 위해 충분한 CPU와 네임노드와 비슷한 용량의 메모리가 필요하므로 별도의 물리머신에서 실행되는게 좋음
      >         * 주 네임노드에 장애날것을 대비해 네임스페이스 이미지 복제본 보관
      >            <br>→ but, 약간의 시간차를 두고 복제되므로 네임노드 장애발생시, 어느정도의 데이터 손실 불가피
3. 블록 캐싱
   + 데이터노드는 디스크에 저장된 블록을 읽는데, 빈번하게 접근하는 블록 파일은 오프힙(off-heap : 자바 힙 외부에서 관리되는) 블록캐시라는 데이터 노드의 메모리에 명시적으로 캐싱 가능
   + 블록은 기본적으로 하나의 데이터노드 메모리에만 캐싱되지만 파일단위로 설정 가능
   + 잡 스케줄러(맵리듀스, 스파크 등)는블록이 캐싱된 데이터노드에에서 태스크가 실행되도록 할 수 있으며, 이를 이용해 성능 높일 수 있음
   + 사용자나 애플리케이션은 캐시 풀(cache pool)에 캐시 지시자(cache directive)를 추가해 특정 파일을 캐싱하도록 명령가능
   + 캐시 풀은 캐시 권한이나 자원의 용도를 관리하는 관리 그룹의 역할을 맡음
4. HDFS 페더레이션
   + 네임노드는 파일시스템의 모든 파일과 각 블록에 대한 참조 정보를 메모리에서 관리
      <br>→ 메모리는 파일이 많은 대형 클러스터의 확장성에 가장 큰 걸림돌
      <br>→ HDFS 페더레이션(연합체)를 활용해 각 네임노드가 파일시스템의 네임스페이스 일부를 나누어 관리하는 방식으로 새로운 네임노드 추가 가능
   + HDFS 페더레이션 적용하면, 각 네임노드는 네임스페이스의 메타데이터를 구성하는 페이스 볼륨과 네임스페이스에 포함된 파일의 전체 블록을 보관하는 블록 풀을 관리
     - 네임스페이스 볼륨은 서로 독립
       <br>→ 네임노드는 서로 통신 X, 특정 네임노드 장애 발생해도 영향 X
       <br>→ But 블록풀의 저장소는 분리되있지 않음
     - 모든 데이터 노드는 클러스터의 각 네임노드마다 등록되있고, 여러 블록풀로부터 블록 저장
5. HDFS 고가용성
   + 데이터 손실 방지를 위해 네임노드 메타데이터를 다수 파일시스템에 복제하는 방식과 보조 네임노드를 사용해 체크포인트를 생성하는 방식 조합해 사용가능
     <br>→ 파일시스템의 고가용성을 보장하진 않음
   + 네임노드에 고장 발생하면 파일 읽기 쓰기 불가
     - 장애 복구위해서 파일시스템 메타데이터 복제본을 가진 새로운 네임노드 구동하고 알려주면 됨
     - But 30분 이상 걸리는 경우도 있고 그동안 어떤 요청도 처리 못함
   + So, HDFS 고가용성(HA : High Availability) 지원 : 활성대기 상태로 설정된 한 쌍의 네임노드로 구현
     - 활성 네임노드에 장애 발생하면 대기 네임노드가 역할을 이어받아 큰 중단 없이 요청 처리
     - 이런 방식 지원하기위해 HDFS의 구조 일부 변경
       > - 네임노드는 에디트 로그를 공유하기 위해 고가용성 공유 스토리지 반드시 사용
       >     <br>→ 

#### 3.3 가나다라마



출처) 톰 화이트,『하둡 완벽 가이드 데이터의 숨겨진 힘을 끌어내는 최고의 클라우드 컴퓨팅 기술』, 장형석, 장정호, 임상배, 김훈동 옮김, 한빛미디어(2017), p54~
