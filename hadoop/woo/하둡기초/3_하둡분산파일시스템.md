### 3장 하둡 분산 파일시스템
+ 분산 파일 시스템 : 네트워크로 연결된 여러 머신의 스토리지를 관리하는 파일 시스템
   <br>→ 일반적인 디스크 파일시스템보다 복잡
+ HDFS(Hadoop Distributed File System) : 하둡의 대표적인 파일 시스템  

#### 3.1 HDFS 설계
+ HDFS의 특징
  - <B style="color:skyblue">매우 큰 파일</B> : 수백 메가바이트, 기가바이트 또는 테라바이트 크기의 파일을 의미, 최근에는 페타바이트 크기의 데이터를 저장하는 하둡클러스터도 존재
  - <B style="color:skyblue">스트리밍 방식의 데이터 접근</B> : HDFS는 가장 효율적인 데이터 처리 패턴은 한 번 쓰고 여러번 읽는 것이라는 아이디어에서 출발했고, 분석이 전부는 아니지만 첫번째 레코드를 읽는데 걸리는 시간보다 전체 데이터셋을 모두 읽을때 걸리는 시간이 더 중요
  - <B style="color:skyblue">범용 하드웨어</B> : 하둡은 고가의 신뢰도 높은 하드웨어만을 고집하지 않고, 범용 하드웨어로 구성된 대형 클러스터에서 장애가 발생해도 사용자가 그 사실을 모르게 작업을 수행하도록 설계됨
+ HDFS가 맞지 않는 응용분야
  - <B style="color:skyblue">빠른 데이터 응답 시간</B> : HDFS는 높은 데이터 처리량을 제공하기 위해 최적화되있고 이를 위해 빠른 응답시간을 포기
  - <B style="color:skyblue">수많은 작은 파일</B> : 네임노드는 파일시스템의 메타데이터를 메모리에서 관리하여 저장가능한 파일 수는 네임노드의 메모리 용량에 좌우되어 수십억 개의 파일은 하드웨어의 용량을 넘어서게 됨
  - <B style="color:skyblue">다중 라이터와 파일의 임의 수정</B> : HDFS는 단일 라이터로 파일을 쓰고, 한번 쓰고 끝나거나 파일의 끝에 덧붙이는 것은 가능하지만 파일에서 임의 위치에 있는 내용을 수정하는 것은 허용하지 않으며 다중 라이터도 지원하지 않음(하둡 3.0부터는 다중 라이터 지원)

#### 3.2 HDFS 개념
1. 블록
   + 블록 크기 : 한 번에 읽고 쓸 수 있는 데이터의 최대량
     - 파일시스템의 블록 크기는 보통 수 kb 고, 디스크 블록의 크기는 기본적으로 512 byte
     - HDFS 블록 크기는 기본적으로 128 MB로 굉장히 큰 단위
   + HDFS 파일은 특정 블록 크기의 청크로 쪼개지고 각 청크는 독립적으로 저장됨
   + 단일 디스크를 위한 파일 시스템은 디스크 블록 크기보다 작은 데이터라도 한 블록 전체를 점유하지만, HDFS는 모두 점유하지 않음
   + HDFS 블록이 큰 이유 : 탐색 비용을 최소화하기 위해서
     - 블록이 매우 크면 블록의 시작점을 탐색하는데 걸리는 시간을 줄이고 데이터를 전송하는데 더 많은 시간 할애 가능
   + 분산 파일 시스템에 블록 추상화의 개념 도입시 얻게되는 이득
        >   1. 파일 하나의 크기가 단일 디스크의 용량보다 커짐
        >       - 한 파일을 구성하는 여러 블록이 동일 디스크에만 저장될 필요가 없음
        >   2. 스토리지의 서브시스템을 단순하게 만들 수 있음 
        >       - 블록은 고정크기고 저장에 필요한 디스크 용량만 계산하면 됨
        >       - 블록은 단지 저장된 데이터의 청크일뿐이고 메타데이터는 별도 시스템으로 분리 가능
   + 블록은 내고장성(fault tolerance)과 가용성(availability)을 제공하는데 필요한 복제(replication) 구현시 적합
      - 블록의 손상과 디스크 및 머신의 장애에 대처하기 위해 각 블록은 물리적으로 분리된 다수의 머신(보통 3개)에 복제됨
      - 블록이나 머신이 손상되면 다른 복사본을 살아있는 머신에 복제하여 복제 계수를 정상수준으로 돌아오게 함
2. 네임노드와 데이터노드
   + HDFS는 마스터인 하나의 네임노드(namenode)와 워커인 여러 데이터노드(datanode)로 구성
   + 네임노드 : 파일시스템의 네임스페이스 관리
      - 파일시스템 트리와 그 트리에 포함된 모든 파일과 디렉터리에 대한 메타데이터 유지
      - 파일에 속한 모든 블록이 어느 데이터 노드에 있는지 파악
      - but, 블록의 위치 정보는 시스템이 시작할때 모든 데이터노드로부터 받아서 재구성하기 때문에 디스크에 영속적으로 저장되진 않음
   + HDFS 클라이언트는 네임노드와 데이터노드 사이에서 통신하고 파일시스템에 접근
   + 데이터노드 : 파일시스템의 실질적인 일꾼
      - 클라이언트나 네임노드의 요청이 있을때 블록을 저장하고 탐색하며, 저장된 블록 목록을 주기적으로 네임노드에 보고
   +  네임노드가 없으면 파일시스템은 동작하지 않고 네임노드를 실행하는 머신이 손상되면 파일시스템의 어떤 파일도 못찾음
     <br>→ 데이터노드에 블록이 저장되있지만 이런 블록 정보를 이용해 파일을 재구성할순 없음
     <br>→ 네임노드의 장애복구 기능은 필수적이며, 하둡은 이를 위해 두가지 매커니즘 제공
      >  1. 파일시스템의 메타데이터를 지속적인 상태로 보존하기 위해 파일로 백업
      >  2. 보조 네임노드 운영
      >     - 보조 네임노드의 역할
      >         *  에디트 로그가 너무 커지지 않도록 주기적으로 네임스페이스 이미지를 에디트 로그와 병합하여 새로운 네임스페이스를 만드는것
      >            <br>→ 병합작업을 위해 충분한 CPU와 네임노드와 비슷한 용량의 메모리가 필요하므로 별도의 물리머신에서 실행되는게 좋음
      >         * 주 네임노드에 장애날것을 대비해 네임스페이스 이미지 복제본 보관
      >            <br>→ but, 약간의 시간차를 두고 복제되므로 네임노드 장애발생시, 어느정도의 데이터 손실 불가피
3. 블록 캐싱
   + 데이터노드는 디스크에 저장된 블록을 읽는데, 빈번하게 접근하는 블록 파일은 오프힙(off-heap : 자바 힙 외부에서 관리되는) 블록캐시라는 데이터 노드의 메모리에 명시적으로 캐싱 가능
   + 블록은 기본적으로 하나의 데이터노드 메모리에만 캐싱되지만 파일단위로 설정 가능
   + 잡 스케줄러(맵리듀스, 스파크 등)는블록이 캐싱된 데이터노드에에서 태스크가 실행되도록 할 수 있으며, 이를 이용해 성능 높일 수 있음
   + 사용자나 애플리케이션은 캐시 풀(cache pool)에 캐시 지시자(cache directive)를 추가해 특정 파일을 캐싱하도록 명령가능
   + 캐시 풀은 캐시 권한이나 자원의 용도를 관리하는 관리 그룹의 역할을 맡음
4. HDFS 페더레이션
   + 네임노드는 파일시스템의 모든 파일과 각 블록에 대한 참조 정보를 메모리에서 관리
      <br>→ 메모리는 파일이 많은 대형 클러스터의 확장성에 가장 큰 걸림돌
      <br>→ HDFS 페더레이션(연합체)를 활용해 각 네임노드가 파일시스템의 네임스페이스 일부를 나누어 관리하는 방식으로 새로운 네임노드 추가 가능
   + HDFS 페더레이션 적용하면, 각 네임노드는 네임스페이스의 메타데이터를 구성하는 페이스 볼륨과 네임스페이스에 포함된 파일의 전체 블록을 보관하는 블록 풀을 관리
     - 네임스페이스 볼륨은 서로 독립
       <br>→ 네임노드는 서로 통신 X, 특정 네임노드 장애 발생해도 영향 X
       <br>→ But 블록풀의 저장소는 분리되있지 않음
     - 모든 데이터 노드는 클러스터의 각 네임노드마다 등록되있고, 여러 블록풀로부터 블록 저장
5. HDFS 고가용성
   + 데이터 손실 방지를 위해 네임노드 메타데이터를 다수 파일시스템에 복제하는 방식과 보조 네임노드를 사용해 체크포인트를 생성하는 방식 조합해 사용가능
     <br>→ 파일시스템의 고가용성을 보장하진 않음
   + 네임노드에 고장 발생하면 파일 읽기 쓰기 불가
     - 장애 복구위해서 파일시스템 메타데이터 복제본을 가진 새로운 네임노드 구동하고 알려주면 됨
     - But 30분 이상 걸리는 경우도 있고 그동안 어떤 요청도 처리 못함
   + So, HDFS 고가용성(HA : High Availability) 지원 : 활성대기 상태로 설정된 한 쌍의 네임노드로 구현
     - 활성 네임노드에 장애 발생하면 대기 네임노드가 역할을 이어받아 큰 중단 없이 요청 처리
     - 이런 방식 지원하기위해 HDFS의 구조 일부 변경
       > - 네임노드는 에디트 로그를 공유하기 위해 고가용성 공유 스토리지 반드시 사용
       >     <br>.  대기 네임노드가 먼저 활성화되면 먼저 기존 활성 네임노드 상태를 동기화하기 위해 공유 에디트 로그를 읽고 이어서 활성 네임노드에 새로 추가된 항목도 읽음
       >     <br>. 데이터 노드는 블록 리프트를 두개의 네임노드에 보냄. 볼록 매핑정보는 디스크가 아닌 네임노드의 메모리에 보관되기 때문
       >     <br>. 클라이언트는 네임노드 장애를 사용자에게 투명한 방식으로 처리할 수 있도록 구성해야 함
       >     <br>. 대기 네임노드는 보조 네임노드의 역할 포함하고, 활성 네임노드 네임스페이스의 체크포인트 작업을 주기적으로 수행
     - 고가용성 공유 스트로지를 위해 NFS 필터나 QJM(Quorum Journal Manager) 사용
       > - QJM은 HDFS 전용 구현체로, 고가용성 에디트 로그를 지원하기 위한 목적으로 설계되었으며 HDFS의 권장 옵션
       > - QJM은 저널그룹에서 동작하며, 각 에디트 로그는 전체 저널 노드에 동시에 쓰여짐
       > - 일반적으로 저널노드는 3개, 그 중 1개가 손상되어도 문제 없음 -> 주키퍼와 유사하지만 QJM은 주키퍼를 사용하지 않고도 이런 기능 구현했다는 것이 중요
        -> 물론 HDFS 고가용성은 활성 네임노들르 선출하기 위해 주키퍼 이용
     - 활성 네임노드에 장애발생하면 대기 네임노드는 매우 빨리 기존 네임노드 대체
       > - 활성&대기 네임노드 모두 최신 에디트 로그와 실시간으로 갱신되는 블록 매핑 정보를 메모리에 유지 하기 때문
     - 가끔 활성 네임노드에 장애있을 때 대기 네임노드가 중단된 상태일수도 있는데, 관리자는 단순히 대기 네임노드를 구동하면 그만이다. 그런데 이게 하둡의 표준운영절차라서 고가용성을 사용하지 않는 시스템에 비해 나쁠것도 없다고.... 책에 쓰여있는데..... 뭐 이런 무책임한 설명이 있지 갑자기 뭔소리지????????? 
   + 장애복구와 펜싱
     - 대기 네임노드를 활성화시키는 전환작업은 <b>장애복구 컨트롤러</b>라는 새로운 객체로 관리
     - 장애복구 컨트롤러의 기본 구현체는 단 하나의 네임노드만 활성 상태에 있는 것을 보장하기 위해 주키퍼 이용
     - 각 네임노드는 경량의 장애복구 컨트롤러 프로세스로 네임노드 장애를 감시하고 발생하면 장애복구 지시
     - 장애복구는 정기적인 유지관리를 위해 관리자가 수동으로 초기화 가능 => 우아한 장애복구
     - 우아한 장애복구에선 장애가 발생한 네임노드가 현재 실행되지 않고 있다는 것을 확신하기 어렵
       > - 예를 들어, 네트워크가 느려지는 등의 장애복구를 위한 전환 작업이 시작되어도 기존 활성 네임노드는 여전히 실행중이고 자신이 활성 네임노드라고 생각
       > - 고가용성 구현을 위해 기존 활성 네임노드가 시스템을 손상시키지 않도록 엄청난 노력기울여야하고, 이를 위해 펜싱(fencing)이란 메서드 제공
     - QJM은 한번에 한 네임노드만 에디트 로그에 쓰도록 보장. But 기존 활성 네임노드가 잘못된 정보 줄 수도 있으므로 SSH펜싱 명령어로 네임노드의 프로세스를 확실히 죽이도록 설정하는게 가장 Goooood
     - 네임노드가 공유 스토리지 디렉토리에 접근 불허하는 방법부터 원격관리 명령어로 네트워크 포트 막는 것 등등 다양한 펜싱 메커니즘 존재

#### 3.3 명령행 인터페이스
 - HDFS를 명령행으로 조작하는 방법인데... 우리는 그럴 일 없으므로 생략~~

#### 3.4 하둡 파일시스템
 - 하둡은 너무..... 어려워   

출처) 톰 화이트,『하둡 완벽 가이드 데이터의 숨겨진 힘을 끌어내는 최고의 클라우드 컴퓨팅 기술』, 장형석, 장정호, 임상배, 김훈동 옮김, 한빛미디어(2017), p54~
